---
layout: post
title:  "Encodage des caractÃ¨res"
description: "EÆžá€„Ñ³áƒ«Ó‘à³¨àª¦"
tags: [unicode, ascii, encoding]
---

Nous pourrions rÃ©sumer lâ€™Ã©criture Ã  une suite de symboles. Ces formes, nommÃ©es "caractÃ¨res", regroupent tous les alphabets du monde, les chiffres, la ponctuation et mÃªme les emojis ðŸ˜„.

Dans cet article, nous allons nous intÃ©resser Ã  comment **sont reprÃ©sentÃ©s les caractÃ¨res dans lâ€™informatique**. Comme toutes donnÃ©es informatisÃ©es, les caractÃ¨res sont reprÃ©sentÃ©s par une suite de zÃ©ros et de uns et affichÃ©s sous forme graphique pour que nous ne nous y perdions pas. Cependant, la faÃ§on dâ€™ordonner les zÃ©ros et les uns dÃ©pend de lâ€™encodage utilisÃ©.

Fait surprenant, lâ€™encodage nâ€™est pas prÃ©sent en tant que mÃ©tadonnÃ©e dâ€™un fichier (comme lâ€™est le titre, ou la date de crÃ©ation). Le seul moyen de le connaÃ®tre estâ€¦ **de le deviner** ! Oui vous avez bien lu, quelque chose qui devrait Ãªtre basique est en fait un vrai plat de spaghettis oÃ¹ chacun utilise des normes diffÃ©rentes. Nous allons dÃ©mÃªler tout cela ci-dessous.

Pour cela, nous devons nous Ã©quiper dâ€™un terminal capable de comprendre plusieurs encodages. Sous macOS, lâ€™application native Terminal permet de le faire. Sinon, la commande \*nix `iconv` permet de traduire dâ€™un encodage Ã  un autre.

# Au dÃ©but, lâ€™ASCII

Au dÃ©but des annÃ©es soixante, les caractÃ¨res Ã©taient codÃ©s sur **sept bits** ! Nous pouvions donc reprÃ©senter `2^7 = 128` caractÃ¨res diffÃ©rents. Lâ€™informatique Ã©tant principalement en anglais, il y avait largement la place pour y reprÃ©senter tout lâ€™alphabet latin en majuscule et en minuscule, plus les chiffres et autres caractÃ¨res de ponctuation.

![Table Ascii](/assets/images/ascii-table.png)
*Les cent vingt-huit caractÃ¨res de la table ASCII.*

Le terme officiel pour dÃ©crire cet encodage est `us-ascii` ou encore `iso646`. Cet encodage est simple, chaque **caractÃ¨re** tient sur **un octet**. Parcourir un fichier est aisÃ©, chaque octet lu peut Ãªtre reprÃ©sentÃ© graphiquement !

CrÃ©ons une sÃ©quence ASCII Ã  la main :

```sh
# terminal en us-ascii

$ echo -ne '\x61\x62\x63' > ascii
$ cat ascii
abc
```

Seulement voilÃ , cet alphabet nâ€™en est quâ€™un parmi trois milles autres[^1]. Il a donc fallu trouver un moyen de pouvoir reprÃ©senter ces autres alphabets.

## Lâ€™ASCII Ã©tendu

Avant toute chose, lâ€™ASCII Ã©tendu nâ€™existe pas. Câ€™est un regroupement de diffÃ©rents encodages basÃ©s sur le mÃªme principe.
Lâ€™ASCII se reprÃ©sente sur sept bits, mais un octet contient huit bits, aprÃ¨s calcul, il reste un bit qui nâ€™est pas utilisÃ© ! GrÃ¢ce Ã  celui-ci, câ€™est cent vingt-huit nouveaux caractÃ¨res qui peuvent Ãªtre crÃ©Ã©s !

Je vous prÃ©sente les encodages `iso-8859-1`, `iso-8859â€“2`, `iso-8859-3`, `iso-8859-4`, `iso-8859-5`, `iso-8859-6`, `iso-8859-7`, `iso-8859-8`, `iso-8859-9`, `iso-8859-10`, `iso-8859-11`, `iso-8859-12`, `iso-8859-13`, `iso-8859-14`, `iso-8859-15`. Ã‡a en fait !

Le principe est de **garder** les caractÃ¨res ASCII et d'**ajouter** les caractÃ¨res manquant dans les cent vingt huit restants. Chaque norme correspond donc Ã  une **combinaison dâ€™alphabets**. Par exemple, lâ€™encodage `iso-8859â€“7` contient les caractÃ¨res latins et les caractÃ¨res grecs. Lâ€™encodage `iso-8859-15` contient les caractÃ¨res latins et les caractÃ¨res manquants des langues europÃ©ennes.

Ceci implique quâ€™un **mÃªme fichier** peut Ãªtre **lu diffÃ©remment** selon lâ€™encodage utilisÃ© pour dÃ©coder, exemple :

```sh
$ echo -ne '\x34\x32\xa5' > ambiguous
```

Je configure mon terminal pour afficher les caractÃ¨res en `iso-8859-1` et jâ€™obtiens :

```sh
$ cat ambiguous
42Â¥
```

Je le configure maintenant en `iso-8859-7` :

```sh
$ cat ambiguous
42â‚¯
```

Ainsi, selon lâ€™encodage je peux parler dâ€™un prix en devise japonaise ou en devise grecque (avant lâ€™euro) et impossible de trancher si je nâ€™ai aucune idÃ©e duquel utiliser.

Outre le fait de devoir savoir quel encodage utiliser, il nâ€™y a tout simplement pas la place pour reprÃ©senter certains alphabets, comme par exemple, les langues utilisant des idÃ©ogrammes.

# Lâ€™Unicode Ã  la rescousse

Dans les annÃ©es quatre-vingt-dix apparaÃ®t le standard **Unicode**, celui-ci cherche Ã  **rassembler tous** les caractÃ¨res dans une **seule norme**. Il contient quelques cent vingt-huit mille caractÃ¨res. Le principe est simple, **un nombre correspond Ã  un symbole**, point.

Ces nombres, appelÃ©s **point de code** sont reprÃ©sentÃ©s en hexadÃ©cimal. Ainsi, le symbole latin `a` a pour point de code `U+0061` et le symbole grec `â„¦` a pour point de code `U+03A9`. Ceux-ci sont agencÃ©s en **plans**, les plus utilisÃ©s Ã©tant le *"plan 0 : Basic Multilingual Plane"* et le *"plan 1 : Supplementary Multilingual Plane"*.

Maintenant, comment encoder ces symboles qui ne peuvent tenir sur un octet ? Faisons au plus simple : prenons notre symbole, trouvons son point de code, Ã©crivons-le sur autant dâ€™octets que nÃ©cessaire, et recommenÃ§ons avec le symbole suivant.

Je vous propose la sÃ©quence dâ€™octets suivante `61 62 63`, pouvez-vous me reconstituer la phrase ?

Bien sÃ»r que non ! Cette suite pourrait correspondre Ã  bien des choses :

```
abc
æ…¢c
aæ‰£
```

Il faut trouver un moyen de reprÃ©senter nâ€™importe quel caractÃ¨re sans ambiguÃ¯tÃ© !

## UTF-8, UTF-16 et UTF-32

Avant toute chose, sachez que ces trois encodages permettent de reprÃ©senter lâ€™**ensemble** des caractÃ¨res Unicode. (Donc ne faites pas mon erreur de penser que lâ€™UTF-32 peut reprÃ©senter plus de caractÃ¨re que lâ€™UTF-16 qui lui-mÃªme peut reprÃ©senter plus de caractÃ¨res que lâ€™UTF-8 ðŸ˜…)

La "seule" diffÃ©rence, est la faÃ§on dâ€™agencer les zÃ©ros et les uns.

### UTF-32

Le plus simple des trois, lâ€™UTF-32 reprÃ©sente chaque caractÃ¨re sur quatre octets ! Ã€ la maniÃ¨re dâ€™un fichier ASCII, ici nous lisons le fichier quatre octets par quatre octets.

```sh
# terminal en utf32

$ echo -ne '\x00\x00\x00\x61\x00\x00\x62\x63' > utf32
$ cat utf32
aæ‰£

$ echo -ne '\x00\x00\x00\x61\x00\x00\x00\x62\x00\x00\x00\x63' > utf32b
$ cat utf32b
abc
```

Aucune ambiguÃ¯tÃ© possible. Seulement voilÃ , les caractÃ¨res qui auparavant tenaient sur un octet, en occupent quatre maintenant, une Ã©norme perte de place !

### UTF-16

Cet encodage permet de faire tenir tout caractÃ¨re sur un **codet** (deux octets) ou un **demi-codet** (deux fois deux octets) (oui ce nâ€™est pas logique).

Les caractÃ¨res du plan zÃ©ro sont reprÃ©sentÃ© sur un codet. Pour les caractÃ¨res de plan supÃ©rieur, nous les reprÃ©sentons sur un demi-codet et devons utiliser les **demi-zones dâ€™indirection**. Ces zones regroupent tous les codets dont les cinq bits de poids fort sont `11011`.

Pour encoder les caractÃ¨res sur un demi-codet nous utilisons la procÃ©dure suivante :

1. soustraire au point de code la valeur `0x10000` ;
2. prendre les dix bits de poids fort et y prÃ©-fixer `1101 10` ;
3. prendre les dix bits de poids faible et y prÃ©-fixer `1101 11`.

Exemple pour `ðŸ˜€` :

```
0000 1111 0110 0000 0000      # 0x1F600 - 0x10000
fort   = 1101 1000 0011 1101  # 1101 10 + 0000 1111 01
faible = 1101 1110 0000 0000  # 1101 11 + 10 0000 0000
1101 1000 0011 1101 1101 1110 0000 0000  # demi-codet
D8        3D        DE        00         # en hexadÃ©cimal
```

Et vÃ©rifions avec :

```sh
# terminal en utf16

$ echo -ne '\xD8\x3D\xDE\x00' > utf16
$ cat utf16
ðŸ˜€
```

Simple nâ€™est-ce pas ? Et si nous pouvions faire encore mieux ?

### UTF-8

Ce dernier encodage permet de reprÃ©senter tous les caractÃ¨res sur un Ã  quatre octets :

- de `U+0000` Ã  `U+007F` il faut **un** octet ;
- de `U+0080` Ã  `U+07FF` il faut **deux** octets ;
- de `U+0800` Ã  `U+FFFF` il faut **trois** octets ;
- Ã  partir de `U+10000` il faut **quatre** octets.

Pour lâ€™encodage, nous nous rÃ©fÃ©rons Ã  cette liste[^2] :

```
0x00000000 - 0x0000007F:
    0xxxxxxx
0x00000080 - 0x000007FF:
    110xxxxx 10xxxxxx
0x00000800 - 0x0000FFFF:
    1110xxxx 10xxxxxx 10xxxxxx
0x00010000 - 0x001FFFFF:
    11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
```

Il nous suffit de substituer les `x` par le code binaire, en commenÃ§ant par les bits de poids faible.

Reprenons notre caractÃ¨re `ðŸ˜€`. Pour lâ€™encoder en UTF-8 nous faisons :

```
      00   011111   011000   000000  # point de code en binaire
11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
11110x00 10011111 10011000 10000000  # substitution
11110000 10011111 10011000 10000000  # tous les x restants en 0
F0       9F       98       80        # en hexadÃ©cimal
```

VÃ©rifions avec :

```sh
# terminal en utf8

$ echo -ne '\xF0\x9F\x98\x80' > utf8
$ cat utf8
ðŸ˜€
```

Encore plus simple vous ne trouvez pas ? Cet encodage a plusieurs avantages. Il gaspille moins de place que les deux autres et il est compatible avec la table ASCII.

# DÃ©tection de lâ€™encodage

Jusquâ€™Ã  prÃ©sent nous devions prÃ©ciser quel encodage utiliser pour afficher les sÃ©quences de textes. Mais, en avons-nous vraiment besoin ?

Utilisons la commande \*nix `file` qui permet de dÃ©tecter lâ€™encodage dâ€™un fichier.

Testons sur nos sÃ©quences prÃ©cÃ©dentes.

```sh
$ file ascii
ASCII text

$ file utf32
data

$ file utf16
ISO-8859 text

$ file utf8
UTF-8 Unicode text
```

Seulement deux encodages sont reconnus ðŸ˜± La divination ne marche pas toujours. Pour Ãªtre sÃ»r, nous avons systÃ©matiquement besoin de spÃ©cifier lâ€™encodage. Ceci nâ€™est vraiment pas ergonomique.

Existe-t-il un moyen de spÃ©cifier cet encodage qui soit transparent Ã  lâ€™utilisateur ?

## Lâ€™indicateur dâ€™ordre des octets

Cet indicateur, appelÃ© *BOM* en anglais, est une sÃ©quence de caractÃ¨res placÃ©e en tÃªte de fichier qui permet de spÃ©cifier lâ€™encodage. Il sâ€™agit dâ€™un nombre magique. Il est prÃ©sent dans le corps du fichier, mais les logiciels ne doivent pas le gÃ©rer en tant que caractÃ¨re, mais en tant quâ€™information (Ã  la maniÃ¨re dâ€™une mÃ©tadonnÃ©e).

En voici une liste (non-exhaustive) :

- UTF-8 : `EF BB BF` ;
- UTF-16 : `FE FF` ;
- UTF-32 : `00 00 FE FF`.

Testons cela.

```sh
$ echo -ne '\x00\x00\xFE\xFF\x00\x00\x00\x61\x00\x00\x62\x63' > utf32
$ echo -ne '\xFE\xFF\xD8\x3D\xDE\x00' > utf16
$ echo -ne '\xEF\xBB\xBF\xF0\x9F\x98\x80' > utf8
```

VÃ©rifions avec :

```sh
$ file utf32
UTF-32 Unicode text

$ file utf16
UTF-16 Unicode text

$ file utf8
UTF-8 Unicode text (with BOM)
```

Tout fonctionne !

Cependant, comment faire pour traiter des flux de donnÃ©es ? Et bien il nâ€™y a pas de solution miracle. Il faut encore spÃ©cifier manuellement lâ€™encodage.

# Au final

Quel encodage devons-nous utiliser ?

**Et bien il nâ€™y a pas de rÃ©ponse.**

Par exemple les langages Java et JavaScript utilisent de lâ€™UTF-16 pour les chaÃ®nes de caractÃ¨res, le langage Rust de lâ€™UTF-8. Microsoft utilise de lâ€™UTF-32 avec indicateur. Les systÃ¨mes \*nix utilisent de lâ€™UTF-8 et de lâ€™UTF-32. Le web repose sur lâ€™UTF-8.

Aujourdâ€™hui, le monde informatique tend Ã  privilÃ©gier lâ€™UTF-8. Ã€ vous de voir ce qui convient le mieux pour vos besoins.

***

RÃ©fÃ©rences :
- [Convertisseur de code Unicode](http://hapax.qc.ca/conversion.fr.html)
- [UTF-8, UTF-16, UTF-32 & BOM](http://www.unicode.org/faq/utf_bom.html)
- [Manually converting unicode codepoints into UTF-8 and UTF-16](https://stackoverflow.com/questions/6240055/manually-converting-unicode-codepoints-into-utf-8-and-utf-16)
- [Character Sets](https://www.iana.org/assignments/character-sets/character-sets.xhtml)

***

[^1]: [How many languages in the world are unwritten](https://www.ethnologue.com/enterprise-faq/how-many-languages-world-are-unwritten-0)
[^2]: [utf-8 - Linux man page](https://linux.die.net/man/7/utf-8)
